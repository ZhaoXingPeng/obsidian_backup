{
	"nodes":[
		{"id":"bc5204a4eade1d70","type":"text","text":"the Micro-Doppler Swin Transformer human\npose estimation (MDST-HPE)","x":-240,"y":370,"width":400,"height":50},
		{"id":"1e9fce37e27d3ef5","type":"text","text":"## MD-Pose","x":420,"y":524,"width":520,"height":700,"color":"5"},
		{"id":"c53308f58ed356f4","type":"text","text":"## Three-Dimensional HPE","x":1020,"y":524,"width":520,"height":700,"color":"5"},
		{"id":"699d670f996bb36a","type":"text","text":"## SCRP-Radar\n首先，将微多普勒谱图输入一个基础神经网络（文中使用的是HRNet），生成特征图。然后，通过一个全连接层（每个关节点使用不同的全连接参数），得到17个关节点的特征嵌入向量。本文的创新点在于其采用了一种新的‘空间感知坐标表示’方法。传统的方法通常是通过热力图直接回归获取x、y坐标，这是一个回归问题；然而，文中提出的方法将x和y坐标的预测分别转化为两个分类问题。\n‘空间感知坐标表示’具体指的是SimCC (Simultaneous Coordinate Classification) 方法，它通过将每个关节点的x和y坐标预测分别看作两个独立的分类任务来解决人体姿态估计问题。首先，沿x轴和y轴分别将空间区域划分为多个离散的区间，每个区间代表一个分类标签。接着，通过神经网络对每个区间的概率进行预测，得到关于x和y坐标的概率分布向量。这样的设计可以更好地捕捉微小位置变化的细节信息，增强模型的精度与鲁棒性。\n在获得x和y坐标的概率向量后，我们需要解码出每个关节点的坐标。解码的方式是通过对这些概率向量进行加权平均计算，得出每个关节点的x和y坐标预测值。具体而言，将每个分类区间的中心坐标与其概率相乘并求和，得到最终的坐标预测。这个过程对每个关节点的x和y坐标分别进行，最终得到17组精确的关节点坐标预测值。","x":-840,"y":524,"width":500,"height":596,"color":"5"},
		{"id":"34e8042c4bd9420b","type":"file","file":"图片/Pasted image 20240911185305.png","x":-840,"y":1224,"width":500,"height":124,"color":"5"},
		{"id":"a81a9131b7a7a7a7","type":"file","file":"图片/Pasted image 20240911185545.png","x":-240,"y":1248,"width":540,"height":200},
		{"id":"1f0ed0c7359d5131","type":"text","text":"首先，Transformer 最早应用于序列到序列 (seq2seq) 任务，例如机器翻译。其主要特点是通过自注意力机制来捕捉序列中不同位置的全局依赖关系而无需像 RNN 或 CNN 那样引入递归或卷积操作​。\n\n后来，Vision Transformer (ViT) 被提出并应用于图像分类任务。\nViT 的核心思想是将输入图像划分为多个固定大小的图像块（patch），ViT在输入序列中添加一个可学习的分类标记（[class] token）。每个 patch 被展平后通过线性投影映射到一个固定维度的向量空间，形成一系列的 patch embeddings。与自然语言处理中的词嵌入类似，这些 patch embeddings 再加上位置编码（position embeddings），共同作为 Transformer 的输入序列。\n\n然而，ViT 也存在一些问题，比如在小数据集上表现较差，因为其模型结构缺乏对局部特征的专门处理。Swin Transformer 在 ViT 的基础上引入了“窗口”这一概念，将自注意力操作限制在局部的非重叠窗口内，从而保留了局部性。同时，窗口可以通过“滑动”机制进行移动，这使得 Swin Transformer 能够逐步建立跨窗口的连接，扩大感受野。此外，Swin Transformer 通过 Patch Merging 机制逐层减少 token 的数量，使得计算效率更高​(Vision Transformers (Vi…)。相比之下，Swin Transformer 在保留全局信息的同时，更好地捕捉了局部特征。","x":-240,"y":1460,"width":540,"height":540,"color":"4"},
		{"id":"537b91f0ed1ee15c","type":"file","file":"图片/Pasted image 20240912153424.png","x":-240,"y":2020,"width":540,"height":159,"color":"4"},
		{"id":"c29814f880f108f9","type":"file","file":"图片/Pasted image 20240912201914.png","x":-240,"y":2200,"width":540,"height":147,"color":"4"},
		{"id":"1cd7fd190591a79f","type":"text","text":"W-MSA(节省计算量)","x":340,"y":2733,"width":220,"height":60,"color":"4"},
		{"id":"e425d09f347281ec","type":"file","file":"图片/Pasted image 20240912202942.png","x":-240,"y":2360,"width":540,"height":301,"color":"4"},
		{"id":"d8d677bb0177c1e3","type":"file","file":"图片/Pasted image 20240912202627.png","x":-240,"y":2661,"width":540,"height":205,"color":"4"},
		{"id":"473759f8bca4555a","type":"file","file":"图片/Pasted image 20240912203509.png","x":-240,"y":2880,"width":540,"height":282,"color":"4"},
		{"id":"adab3f450c1c5b78","type":"text","text":"SW-MSA","x":340,"y":2991,"width":140,"height":60,"color":"4"},
		{"id":"b11f530085ca3d8d","type":"file","file":"图片/Pasted image 20240912204118.png","x":580,"y":3140,"width":440,"height":206,"color":"4"},
		{"id":"1f074c374337a7f0","type":"text","text":"计算量增加怎么处理","x":660,"y":3380,"width":250,"height":60,"color":"4"},
		{"id":"5f79cd0f8b3522ed","type":"file","file":"图片/Pasted image 20240912204923.png","x":1060,"y":3140,"width":720,"height":209,"color":"4"},
		{"id":"7f71db3ec895581d","type":"text","text":"避免信息混淆","x":1240,"y":3380,"width":250,"height":60,"color":"4"},
		{"id":"194a6d88475712fe","type":"file","file":"图片/Pasted image 20240912203143.png","x":611,"y":2661,"width":379,"height":205,"color":"4"},
		{"id":"636e8f9408a9aa9f","type":"file","file":"图片/Pasted image 20240912203736.png","x":581,"y":2928,"width":439,"height":187,"color":"4"},
		{"id":"2953f2394c33546a","type":"text","text":"首先，Vision Transformer（ViT）提出了一个创新的想法，将图像划分为多个小块（patch），然后应用 Transformer 来处理计算机视觉任务。虽然 ViT 的设计在图像分类等任务上取得了不错的效果，但它在处理高分辨率图像和整体特征提取方面遇到了一些问题。主要原因是当图像分辨率很高时，patch 的数量会急剧增加，导致计算注意力机制（self-attention）的复杂度成倍增加，进而使得计算成本变得非常高。","x":320,"y":1460,"width":502,"height":204,"color":"4"},
		{"id":"42f1d2f033a94c8a","type":"text","text":"接下来我们谈谈 Swin Transformer 是如何改进这些问题的。Swin Transformer 的核心改进在于如何更高效地处理这些图像特征。首先，它也是将输入图像通过 Patch Partition（补丁分割） 分成固定大小的非重叠小块（patch），每个小块作为一个 token，这与 ViT 类似。接着，这些 patch 会通过 Linear Embedding（线性嵌入） 映射到一个更高的维度 \nC（例如 96 维），这样做的目的是更好地表达每个小块的特征。","x":320,"y":1680,"width":502,"height":220,"color":"4"},
		{"id":"f137b4a2ea0fd977","type":"text","text":"然后，我们进入了 Swin Transformer 的核心部分，也就是 Swin Transformer Block 的处理。这个部分包含了几个特别的设计。首先，Swin Transformer 引入了 W-MSA（Windows Multi-Head Self-Attention） 的机制。在 ViT 中，所有的自注意力（self-attention）计算都是基于整个图像的，这会导致计算量非常大。Swin Transformer 的做法则是将特征图划分成多个不相交的小窗口（Window），然后在每个窗口内单独计算自注意力。这样一来，计算复杂度就大大降低了，特别是在浅层特征图非常大的时候，这种方式非常高效。","x":-820,"y":2635,"width":520,"height":256,"color":"4"},
		{"id":"b425f94eb5538ead","type":"text","text":"然而，这种窗口内的自注意力计算（W-MSA）也带来了一个问题：不同窗口之间的信息无法传递。为了解决这个问题，Swin Transformer 引入了 Shifted Windows Multi-Head Self-Attention (SW-MSA)。SW-MSA 的核心思想是通过在相邻层中对窗口进行移动和重组，使得信息可以在相邻窗口之间传递，而不会增加计算量，也避免了模型学到错误的位置信息。简单来说，SW-MSA 让模型能够看到更多的上下文信息，而不仅仅局限于一个小窗口之内。","x":-820,"y":2895,"width":520,"height":252,"color":"4"},
		{"id":"06407e6984eb5ace","type":"text","text":"接着，在每个 Stage 中，Swin Transformer 使用 Patch Merging（补丁合并） 操作来逐步减少补丁的数量（降低分辨率），同时增加特征图的通道数。这使得模型可以在更高层次的表示中整合更丰富的特征信息。每个 Stage 都包含多个 Swin Transformer Block，逐渐提取图像的高级特征。","x":368,"y":2421,"width":425,"height":180,"color":"4"},
		{"id":"d8108064d3fa92e2","type":"text","text":"## MDST\n在swin transformer上面的创新操作,级联（Cascade）和并行（Parallel）结构与原始的Swin Transformer的相似性与不同： \n- **级联（Cascade）结构**与原始的Swin Transformer的方法相似，都采用了在一个**stage**中**串联多个Transformer块（block）的方式。 在这种结构中，每个**block**的输出作为下一个**block**的输入，逐层累积特征。这种设计有助于逐步提取和丰富特征信息，使得模型能够在每一层上都学习到更加高级的特征。\n- 并行（Parallel）结构的不同之处： **并行（Parallel）结构**则是**在一个stage中并联多个Transformer块（block）**。在这种配置中，不同的**block**可以在同一层级上同时处理不同的特征尺度（例如，一个block专注于局部特征，另一个block专注于全局特征）。\n倒特征金字塔网络（IFPN，Inverted Feature Pyramid Network）是一种在论文中用来增强人体姿态估计模型特征提取能力的技术：\n- **多层特征提取**：\n    - 在人体姿态估计中，不同的关节点可能有不同的特征尺度（比如，手部细节需要更细的特征，而整体身体姿态则需要更大尺度的特征）。IFPN将这些不同层级的特征进行提取和处理，以便模型可以同时处理细节信息和整体信息。\n- **特征上采样和融合**：\n    - IFPN通过**上采样（upsampling）**将低分辨率的特征图转换为高分辨率的特征图。这样做的目的是在高分辨率层级上融合来自不同尺度的特征信息。\n    - 通过这种方式，IFPN可以把来自不同深度（不同尺度）的特征信息整合到一起，确保在高层次的特征图中既有全局的语义信息，也有细粒度的空间信息。\n- **多尺度监督和回归**：\n    - 在人体姿态估计的过程中，不仅要判断人体的整体位置，还要精确定位每个关节点的位置。IFPN通过在不同的尺度上进行监督学习，使得模型能够从多种角度来学习特征，这样可以更准确地估计人体姿态。\n","x":-240,"y":260,"width":540,"height":964,"color":"5"},
		{"id":"a43ff52b9207c3de","type":"file","file":"图片/Pasted image 20240911185225.png","x":-240,"y":-720,"width":540,"height":156,"color":"5"},
		{"id":"742904ad23150ab1","type":"text","text":"## 数据处理过程\n首先，雷达采集到的一维（1D）回波数据是时间序列形式的，这些数据被记录为`slow time`（慢时间）和`fast time`（快时间）信号。`fast time`表示雷达信号在一个脉冲内的采样，用于描述目标的距离信息；而`slow time`则对应多个脉冲的采样时间序列，描述的是目标的速度信息。通过将这些一维数据按照`slow time`和`fast time`的关系排列形成一个二维数据矩阵，其中行表示`fast time`，列表示`slow time`。\n\n接下来，论文采用二维傅里叶变换（2D FFT）来将这个二维数据矩阵从时域转换到频域，生成距离-多普勒图像。在这个图像中，横轴代表距离（由`fast time`信息决定），纵轴代表多普勒频移（由`slow time`信息决定），这可以反映出目标的运动速度信息。\n\n然后，为了获得更直观的微多普勒特征谱图，论文会对距离-多普勒图像进行能量累积处理。具体来说，通过沿着距离轴（`fast time`方向）叠加能量，可以生成一个新的图像，这个图像的横轴表示时间（帧数），纵轴表示微多普勒频谱的强度。这种微多普勒特征谱图捕捉了目标在运动过程中的细节变化特征。\n\n到此为止，数据处理步骤完成了从雷达回波信号到微多普勒特征谱图的转化。随后，这些谱图作为输入数据，后续问题则转化成计算机视觉（CV）问题，通过深度学习方法进行进一步的人体姿态估计等任务处理。","x":-240,"y":-540,"width":540,"height":620,"color":"5"},
		{"id":"733e9c00bc6277d0","type":"text","text":"### 评估指标\n\n1. **平均每关节位置误差（MPJPE, Mean Per-Joint Position Error）**：MPJPE是衡量人体姿态估计精度的主要指标。它通过计算所有测试姿势下预测的关节位置与真实关节位置之间的平均欧几里德距离来评估模型的准确性。\n\n2. **正确关键点百分比（PCK, Percentage of Correct Keypoints）**：PCK是另一个用于衡量训练过程有效性的指标。它计算估计的人体关键点与真实值之间归一化距离小于预定义阈值（本文设定为0.7）的正确比例。\n\n3. **均值和方差（Mean and Variance）**：在比较模型性能时，研究报告了每个模型在不同测试姿势下的误差均值和方差。均值代表模型的平均预测误差，而方差则衡量误差的离散程度，反映了模型在不同场景中的稳定性。\n\n4. **最大误差和最小误差（Maximum and Minimum Errors）**：除了均值和方差外，研究还记录了每个模型的最大和最小误差。最大误差显示了模型在最差情况下的表现，而最小误差则显示了模型在最佳情况下的准确性。","x":-1420,"y":524,"width":460,"height":596,"color":"5"},
		{"id":"5ba73c630779ed3e","type":"file","file":"图片/Pasted image 20240913152749.png","x":-1420,"y":1139,"width":460,"height":251,"color":"5"},
		{"id":"6ea08c32c39aeb79","type":"file","file":"图片/Pasted image 20240913153703.png","x":-1420,"y":1420,"width":460,"height":247,"color":"5"},
		{"id":"3328d984f7d22e6a","type":"text","text":"不同模型和骨干网络的误差比较","x":-1760,"y":1388,"width":300,"height":60,"color":"5"},
		{"id":"a4292122721dc70e","type":"file","file":"图片/Pasted image 20240913154411.png","x":-1420,"y":23,"width":460,"height":457},
		{"id":"7a43f5d0b75db894","type":"file","file":"图片/Pasted image 20240913155933.png","x":-845,"y":2020,"width":499,"height":276},
		{"id":"eafcdf26476a0a1c","type":"file","file":"图片/Pasted image 20240913155847.png","x":-845,"y":1761,"width":505,"height":239},
		{"id":"71bb9362cfb96b92","type":"text","text":"MDST不同的模型组合结果\nT：tiny  B：base\nC：级联 P：并行\nIFPN：倒金字塔","x":-1280,"y":1935,"width":280,"height":130,"color":"5"},
		{"id":"63838cb8265765d7","type":"file","file":"图片/Pasted image 20240913160339.png","x":420,"y":-720,"width":519,"height":260,"color":"5"},
		{"id":"902b61259977845f","type":"file","file":"图片/Pasted image 20240913160359.png","x":420,"y":-440,"width":520,"height":273,"color":"5"},
		{"id":"2384b0eea7ce79f2","type":"text","text":"- **第一幅图**进一步展示了雷达系统的配置及其与人体目标之间的角度关系。图中包含了发射天线和接收天线的位置，它们之间的距离分别为17厘米（X方向）和26厘米（Y方向）。图中显示了目标的方位角（Azimuth Angle, φ）和仰角（Elevation Angle, θ），这两个角度用于描述目标相对于雷达天线的空间方位。目标在不同的方位角和仰角下的运动轨迹可通过雷达系统的接收信号进行捕获。\n- 图中显示了雷达的发射（Tx）和接收（Rx）天线的位置，分别位于坐标系的原点和相对点。图中描绘了人体目标的运动轨迹以及雷达发射与接收天线之间的相对几何关系，包括多个坐标系：全局坐标系 (U, V, W)、雷达参考坐标系 (X, Y, Z)、目标坐标系 (x, y, z)，以及目标运动的相对旋转和变化。这些坐标系的转换和相对运动轨迹（例如 J1J_1J1​ 到 J1′J_1'J1′​）反映了人体目标在雷达视野中的运动。通过分析这些几何关系，可以计算出目标各个关节位置的距离和运动轨迹，进而通过微多普勒效应估计人体姿态。","x":1040,"y":-620,"width":700,"height":360,"color":"5"},
		{"id":"7c96703dd0690631","type":"text","text":"**Human Pose Estimation**（人体姿态估计）是一种计算机视觉任务，旨在通过图像或视频来检测和定位人体的关节（如头部、肩膀、肘部、膝盖等）的位置。它的目标是输出人体骨架的结构，即人体关键点（keypoints）的坐标，通常以二维或三维形式表示。\n**人体动作识别**（Human Action Recognition, HAR）是计算机视觉和机器学习领域的一项任务，旨在通过分析图像序列或视频数据来识别和分类人类的动作或行为。","x":-840,"y":-900,"width":500,"height":220,"color":"4"},
		{"id":"49b305b146bb65bd","type":"text","text":"## 相关工作\n### 1. 基于摄像头的HPE方法\n**优点**：深度学习方法极大地提升了基于摄像头的HPE方法的精度和效率。例如，DeepPose方法使用级联卷积神经网络来逐步回归关节坐标，大幅提高了姿态估计的准确性。\n**缺点**：基于摄像头的方法存在隐私问题和视线限制（如需要在明亮或光照充足的环境下工作）。\n### 2.基于无线感知的人体姿态估计方法：\n**优点**：这种方法利用无线信号（如WiFi、RFID、雷达等）的特性，能够穿透遮挡物并在非视距条件下工作。例如，WiFi和RFID系统能够在隐私保护的环境中提供较为廉价和有效的解决方案，不依赖光照条件。RFID-Pose系统能够实时监测多个关节，具有较小的尺寸和较好的实时性.\n**缺点**：这些系统通常受限于较低的空间分辨率和数据速率。例如，WiFi基于CSI（信道状态信息）的姿态估计方法分辨率较粗，无法捕捉细微的动作；RFID系统的数据速率较低，难以生成高精度的联合置信图。\n### 3.基于雷达的姿态估计方法：\n**优点**：雷达技术，尤其是超宽带（UWB）雷达和毫米波（mmWave）雷达，在不受光线和视线限制的情况下，可以在多种环境中持续工作。\n**缺点**：然而，基于雷达的方法对环境变化比较敏感，周围环境的变化以及人体与雷达的相对距离都会显著影响雷达成像的质量和姿态估计的精确度。","x":-840,"y":-630,"width":500,"height":800,"color":"5"}
	],
	"edges":[
		{"id":"7bc207238e482f74","fromNode":"49b305b146bb65bd","fromSide":"right","toNode":"742904ad23150ab1","toSide":"left","color":"5"},
		{"id":"ff5a3a9200bb63fa","fromNode":"742904ad23150ab1","fromSide":"bottom","toNode":"699d670f996bb36a","toSide":"top","color":"5"},
		{"id":"81843c8b4bb739af","fromNode":"742904ad23150ab1","fromSide":"bottom","toNode":"d8108064d3fa92e2","toSide":"top","color":"5"},
		{"id":"367df482050a7104","fromNode":"742904ad23150ab1","fromSide":"bottom","toNode":"1e9fce37e27d3ef5","toSide":"top","color":"5"},
		{"id":"c4be2c8a4e4e4623","fromNode":"742904ad23150ab1","fromSide":"bottom","toNode":"c53308f58ed356f4","toSide":"top","color":"5"},
		{"id":"701f1fbd49c639fe","fromNode":"a81a9131b7a7a7a7","fromSide":"top","toNode":"d8108064d3fa92e2","toSide":"bottom"},
		{"id":"2d5a2256a81bbca3","fromNode":"34e8042c4bd9420b","fromSide":"top","toNode":"699d670f996bb36a","toSide":"bottom"},
		{"id":"befee6163349ff0d","fromNode":"a81a9131b7a7a7a7","fromSide":"bottom","toNode":"1f0ed0c7359d5131","toSide":"top"},
		{"id":"a5ccb139af26f423","fromNode":"537b91f0ed1ee15c","fromSide":"bottom","toNode":"c29814f880f108f9","toSide":"top"},
		{"id":"17dbc17822a63f43","fromNode":"d8d677bb0177c1e3","fromSide":"right","toNode":"1cd7fd190591a79f","toSide":"left"},
		{"id":"3ff6c6636236f0cf","fromNode":"194a6d88475712fe","fromSide":"left","toNode":"1cd7fd190591a79f","toSide":"right"},
		{"id":"691c68c095e73b10","fromNode":"473759f8bca4555a","fromSide":"right","toNode":"adab3f450c1c5b78","toSide":"left"},
		{"id":"c3e6d5520c488102","fromNode":"adab3f450c1c5b78","fromSide":"right","toNode":"636e8f9408a9aa9f","toSide":"left"},
		{"id":"8ee5b6cd94ad0ce4","fromNode":"b11f530085ca3d8d","fromSide":"right","toNode":"5f79cd0f8b3522ed","toSide":"left"},
		{"id":"6fdcc1b3d78a1c30","fromNode":"f137b4a2ea0fd977","fromSide":"right","toNode":"d8d677bb0177c1e3","toSide":"left"},
		{"id":"b813bddc69d9661f","fromNode":"b425f94eb5538ead","fromSide":"right","toNode":"473759f8bca4555a","toSide":"left"},
		{"id":"cf361584ae9ced75","fromNode":"e425d09f347281ec","fromSide":"right","toNode":"06407e6984eb5ace","toSide":"left"},
		{"id":"65e182cebee2cc6a","fromNode":"3328d984f7d22e6a","fromSide":"top","toNode":"5ba73c630779ed3e","toSide":"left"},
		{"id":"5ac944808c3aef72","fromNode":"3328d984f7d22e6a","fromSide":"bottom","toNode":"6ea08c32c39aeb79","toSide":"left"},
		{"id":"e8cb16d21df4c902","fromNode":"a4292122721dc70e","fromSide":"right","toNode":"699d670f996bb36a","toSide":"top"},
		{"id":"085439ff0a1c7300","fromNode":"71bb9362cfb96b92","fromSide":"top","toNode":"eafcdf26476a0a1c","toSide":"left"},
		{"id":"be320e1d4f21c3fc","fromNode":"71bb9362cfb96b92","fromSide":"bottom","toNode":"7a43f5d0b75db894","toSide":"left"},
		{"id":"8a7a1d4b4b6fc1c9","fromNode":"63838cb8265765d7","fromSide":"right","toNode":"2384b0eea7ce79f2","toSide":"top"},
		{"id":"6149ea8088ed5f77","fromNode":"902b61259977845f","fromSide":"right","toNode":"2384b0eea7ce79f2","toSide":"bottom"}
	]
}