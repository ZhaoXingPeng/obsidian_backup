{
	"nodes":[
		{"id":"d3ed67dd75201bf4","type":"text","text":"src = （128，37）\nsrc_mask = （128，1，1，37）\ntrg = （128，37）\ntrg_mask = （128，1，37，37）\nencoder(src, src_mask)\nemb(src) = x（128，37，512）\nencode->x = self.attention(q=x, k=x, v=x, mask=src_mask)->\nqkv=（128，37，512）\nsplit  = 这个函数接收一个形状为 `[batch_size, length, d_model]` 的张量，将其沿着 `d_model` 维度划分为多个头，每个头的维度为 `d_model / n_head`，并返回一个形状为 `[batch_size, n_head, length, d_tensor]` 的张量。","x":-440,"y":-70,"width":600,"height":310,"color":"4"},
		{"id":"3692e383340cff18","type":"text","text":"- **输入数据**：`(batch_size, src_len)` ——> **输入 token 的索引**\n- **嵌入层**：`(batch_size, src_len)` ——> `(batch_size, src_len, d_model)` ——> **嵌入向量表示**\n- **位置编码**：`(batch_size, src_len, d_model)` ——> `(batch_size, src_len, d_model)` ——> **添加位置信息的嵌入表示**\n- **编码器**：`(batch_size, src_len, d_model)` ——> `(batch_size, src_len, d_model)` ——> **上下文表示**\n- **目标序列嵌入**：`(batch_size, tgt_len)` ——> `(batch_size, tgt_len, d_model)` ——> **目标 token 嵌入表示**\n- **解码器**：`(batch_size, tgt_len, d_model)` 和 `(batch_size, src_len, d_model)` ——> `(batch_size, tgt_len, d_model)` ——> **解码后的表示**\n- **输出层**：`(batch_size, tgt_len, d_model)` ——> `(batch_size, tgt_len, vocab_size)` ——> **预测每个时间步的词汇分布**","x":-440,"y":260,"width":1160,"height":220,"color":"4"},
		{"id":"a129559d5ef33694","type":"text","text":"#### encoder里面emd的参数\n**`d_model`**：词嵌入向量的维度，即每个 token 的向量表示的大小。如果 `d_model=512`，那么每个词会被映射为 512 维度的向量。\n**`max_len`**：最大序列长度，决定位置编码的大小。\n**`vocab_size`**：词汇表大小，嵌入矩阵中的行数，即可以处理的唯一 token 数量。","x":220,"y":-70,"width":500,"height":220,"color":"4"},
		{"id":"b583da027007633b","type":"text","text":"`hidden_size` / `d_model`:表示 Transformer 中每个输入嵌入向量的维度。所有层的输入和输出的嵌入维度保持一致。\n**同义词**：`d_model` `embed_dim` `model_dim` `hidden_dim`（在某些上下文中也被使用，但可能会有歧义）\n `num_heads`:多头注意力机制中的注意力头的数量。\n**同义词**： `nhead` `num_attention_heads`\n `dim_feedforward`：前馈神经网络层中隐藏层的维度，一般比 `d_model` 大得多。\n**同义词**： `ffn_dim` `feedforward_dim` `hidden_ff_dim`（一些实现中为了区分嵌入层）\n`src` / `tgt`：表示输入序列和目标序列的张量。在编码器中是 `src`，在解码器中是 `tgt`。\n**同义词**： `input` / `target`（用于表示输入和目标） `source`（表示源语言序列，在翻译任务中） `x` / `y`（一些简化实现中）\n `attention_mask`：用于屏蔽不需要注意的部分，例如填充部分（padding）或者未来的信息（在自回归生成中）。\n**同义词**： `attn_mask` `mask` `src_mask` / `tgt_mask`（分别表示编码器和解码器部分的掩码）\n`key_padding_mask`：用于标记哪些位置是填充位置，通常用于表示哪些部分在计算注意力时需要被忽略。\n**同义词**： `padding_mask` `mask`（上下文明确时）\n `memory`：编码器的输出，是解码器中需要的上下文表示，作为解码器的输入之一。\n**同义词**： `encoder_output` `context`（特别是在翻译任务中）\n `dropout`：在模型训练过程中，随机丢弃神经元的概率，用于防止过拟合。\n**同义词**： `dropout_rate` `p`（一些实现中为了简洁）\n`LayerNorm` / `BatchNorm`：层归一化或批归一化，用于对模型的输出进行归一化，增强训练的稳定性。\n**同义词**： `norm`（一般情况下，`norm` 指代归一化操作） `layer_norm`（明确指代层归一化）\n `positional_encoding`：位置编码，用于向模型中注入序列的位置信息，使得模型能够区分序列中元素的相对位置。\n**同义词**： `pos_emb`（位置嵌入） `position_embedding` `position_enc`\n `linear1` / `linear2`：通常表示前馈神经网络层中的两个线性变换层，`linear1` 将维度扩展到 `dim_feedforward`，`linear2` 再将其映射回 `d_model`。\n**同义词**： `fc1` / `fc2`（`fc` 表示全连接层，即 fully connected layer） `dense1` / `dense2`（用于表示密集层）\n `output`：表示模型的最终输出，一般是通过解码器层获得的结果。\n **同义词**： `y` / `y_hat`（表示预测值） `logits`（特别是在分类任务中，用于指代经过线性层但未经过 softmax 的输出）","x":-440,"y":500,"width":1160,"height":620,"color":"4"},
		{"id":"e9de333f087900de","x":-440,"y":1140,"width":679,"height":260,"type":"file","file":"图片/Pasted image 20241107144911.png"},
		{"id":"b3ce8acb10c1c150","x":860,"y":-70,"width":560,"height":90,"type":"text","text":"我现在想做一个模型的HPE聚合任务，使用编码器输出动作类别使用解码器输出坐标，可行吗？"},
		{"id":"acdc96052d450a44","x":860,"y":60,"width":560,"height":1940,"type":"text","text":"是的，这种方法是**可行的**，并且已经在许多任务中取得了成功。以下是如何将**动作分类**和**关键点预测**（HPE）聚合在一个模型中的详细思路和步骤。\n\n---\n\n### **1. 任务分解**\n\n- **动作分类**: 预测输入视频或图像中人的动作类别（如走路、跳跃、坐下等）。\n  - **输出**: 一个离散的类别标签。\n\n- **关键点预测（HPE）**: 预测输入中人体的关键点坐标（如关节位置）。\n  - **输出**: 一组连续的坐标值，通常是2D或3D关键点的集合。\n\n---\n\n### **2. 模型架构设计**\n\n你可以采用 **Encoder-Decoder** 结构来处理这两个任务：\n\n#### **(1) 编码器（Encoder）**\n- 负责提取输入图像或序列的全局特征。\n- 输入可以是：\n  - 单帧图像\n  - 视频序列（如3D卷积或时序Transformer）\n  \n- 编码器的输出是一组高维特征表示。\n\n#### **(2) 解码器1：动作分类**\n- 从编码器的特征中提取全局信息。\n- 使用一个简单的线性层（或多层感知机）对动作类别进行分类。\n- **输出**: 动作类别标签。\n\n#### **(3) 解码器2：关键点预测**\n- 解码器根据编码器的特征逐步生成关键点的坐标。\n- 可以使用：\n  - 多层感知机（MLP）：直接预测所有关键点。\n  - Transformer 解码器：逐步预测每个关键点，保证关键点之间的关系。\n\n- **输出**: 每个关键点的2D或3D坐标。\n\n---\n\n### **3. 训练方式**\n\n为了让模型同时学习动作分类和关键点预测，你需要定义一个多任务损失函数。\n\n#### **(1) 损失函数**\n- **动作分类损失**（交叉熵损失）:\n  \\[\n  \\mathcal{L}_{\\text{class}} = \\text{CrossEntropyLoss}(\\hat{y}, y)\n  \\]\n  - \\(\\hat{y}\\): 模型预测的动作类别\n  - \\(y\\): 真实的动作标签\n\n- **关键点预测损失**（均方误差或L1损失）:\n  \\[\n  \\mathcal{L}_{\\text{keypoints}} = \\frac{1}{N} \\sum_{i=1}^{N} ||\\hat{K}_i - K_i||^2\n  \\]\n  - \\(\\hat{K}_i\\): 预测的第 \\(i\\) 个关键点\n  - \\(K_i\\): 真实的第 \\(i\\) 个关键点\n\n- **总损失**:\n  \\[\n  \\mathcal{L} = \\alpha \\mathcal{L}_{\\text{class}} + \\beta \\mathcal{L}_{\\text{keypoints}}\n  \\]\n  其中，\\(\\alpha\\) 和 \\(\\beta\\) 是损失的权重，用于平衡两个任务的影响。\n\n#### **(2) 训练步骤**\n- 输入图像或视频。\n- 编码器提取特征。\n- 动作分类解码器和关键点预测解码器分别处理特征，得到动作类别和关键点坐标。\n- 使用多任务损失函数优化整个模型。\n### **4. 可行性分析**\n\n#### **优势**:\n1. **共享特征表示**:\n   - 编码器提取的特征可以同时用于动作分类和关键点预测，提高特征利用率。\n\n2. **多任务学习的正则化效果**:\n   - 两个任务共享模型部分参数（如编码器），可以起到正则化作用，提升模型泛化能力。\n\n3. **减少推理时间**:\n   - 相比于单独训练两个模型，聚合任务减少了推理的时间和计算成本。"}
	],
	"edges":[
		{"id":"1324fc15ebdf34ec","fromNode":"d3ed67dd75201bf4","fromSide":"right","toNode":"a129559d5ef33694","toSide":"left"}
	]
}